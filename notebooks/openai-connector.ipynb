{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenAI Connector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preamble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import os\n",
    "import getpass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_secret(key):\n",
    "    secret_file = os.path.join(\"../secrets\", f\"{key}.txt\")\n",
    "    with open(secret_file, 'r') as f:\n",
    "        secret = f.readline().strip()\n",
    "    return secret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OpenAIConnector:\n",
    "    def __init__(self, api_key):\n",
    "        self.api_key = api_key\n",
    "        openai.api_key = api_key\n",
    "\n",
    "    def list_models(self):\n",
    "        models = openai.Model.list()\n",
    "        model_names = [model.id for model in models['data']]\n",
    "        return model_names\n",
    "    \n",
    "    def select_model(self, model_name):\n",
    "        self.model_name = model_name\n",
    "        return self\n",
    "\n",
    "    def generate_response(self, prompt):\n",
    "        response = openai.Completion.create(\n",
    "            engine=self.model_name,\n",
    "            prompt=prompt,\n",
    "            max_tokens=100,\n",
    "            n=1,\n",
    "            stop=None,\n",
    "            temperature=0.5,\n",
    "        )\n",
    "        return response.choices[0].text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bot:\n",
    "    pass\n",
    "\n",
    "class TrIAge(Bot):\n",
    "    \"\"\" A helpful bot assisting users and maintainers of open source projects. \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, \n",
    "        model_provider,\n",
    "        api_key, \n",
    "        model_name=\"gpt-3.5-turbo\",\n",
    "    ):\n",
    "        self.name = \"trIAge\"\n",
    "        self.model_provider = model_provider\n",
    "        self.model_name = model_name\n",
    "        self.set_api_key(api_key)\n",
    "        self.chat_history = []\n",
    "        self.configure()\n",
    "\n",
    "    def set_api_key(self, api_key):\n",
    "        if self.model_provider == \"openai\":\n",
    "            self.api_key = api_key\n",
    "            openai.api_key = api_key\n",
    "        else:\n",
    "            raise NotImplementedError(f\"unknown model provider {self.model_provider}\")\n",
    "        \n",
    "    def configure(self):\n",
    "        self.chat_history = []\n",
    "        self.chat_history.append(\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": f\"You are {self.name}, a helpful bot that assists users and maintainers of open source projects.\",\n",
    "            }\n",
    "        )\n",
    "        openai.ChatCompletion.create(\n",
    "            model=self.model_name,\n",
    "            messages=self.chat_history\n",
    "        )\n",
    "    \n",
    "    def tell(\n",
    "        self,\n",
    "        prompt,\n",
    "    ) -> str:\n",
    "        self.chat_history.append({\"role\": \"user\", \"content\": prompt})\n",
    "        response = openai.ChatCompletion.create(\n",
    "            model=self.model_name,\n",
    "            messages=self.chat_history\n",
    "        )\n",
    "        message = response.choices[0].message\n",
    "        answer = message[\"content\"]\n",
    "        self.chat_history.append(message)\n",
    "        print(answer)\n",
    "\n",
    "    def read_issue(self, issue):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = get_secret(\"openai_token\")\n",
    "openai_connector = OpenAIConnector(api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['babbage',\n",
       " 'davinci',\n",
       " 'babbage-code-search-code',\n",
       " 'text-similarity-babbage-001',\n",
       " 'text-davinci-001',\n",
       " 'ada',\n",
       " 'curie-instruct-beta',\n",
       " 'babbage-code-search-text',\n",
       " 'babbage-similarity',\n",
       " 'gpt-3.5-turbo',\n",
       " 'code-search-babbage-text-001',\n",
       " 'gpt-3.5-turbo-0301',\n",
       " 'code-cushman-001',\n",
       " 'code-search-babbage-code-001',\n",
       " 'text-ada-001',\n",
       " 'text-embedding-ada-002',\n",
       " 'text-similarity-ada-001',\n",
       " 'text-davinci-insert-002',\n",
       " 'code-davinci-002',\n",
       " 'ada-code-search-code',\n",
       " 'ada-similarity',\n",
       " 'whisper-1',\n",
       " 'text-davinci-003',\n",
       " 'code-search-ada-text-001',\n",
       " 'text-search-ada-query-001',\n",
       " 'text-curie-001',\n",
       " 'text-davinci-edit-001',\n",
       " 'davinci-search-document',\n",
       " 'ada-code-search-text',\n",
       " 'text-search-ada-doc-001',\n",
       " 'code-davinci-edit-001',\n",
       " 'davinci-instruct-beta',\n",
       " 'text-similarity-curie-001',\n",
       " 'code-search-ada-code-001',\n",
       " 'ada-search-query',\n",
       " 'text-search-davinci-query-001',\n",
       " 'curie-search-query',\n",
       " 'davinci-search-query',\n",
       " 'text-davinci-insert-001',\n",
       " 'babbage-search-document',\n",
       " 'ada-search-document',\n",
       " 'text-search-curie-query-001',\n",
       " 'text-search-babbage-doc-001',\n",
       " 'text-davinci-002',\n",
       " 'curie-search-document',\n",
       " 'text-search-curie-doc-001',\n",
       " 'babbage-search-query',\n",
       " 'text-babbage-001',\n",
       " 'text-search-davinci-doc-001',\n",
       " 'text-search-babbage-query-001',\n",
       " 'curie-similarity',\n",
       " 'curie',\n",
       " 'text-similarity-davinci-001',\n",
       " 'davinci-similarity',\n",
       " 'cushman:2020-05-03',\n",
       " 'ada:2020-05-03',\n",
       " 'babbage:2020-05-03',\n",
       " 'curie:2020-05-03',\n",
       " 'davinci:2020-05-03',\n",
       " 'if-davinci-v2',\n",
       " 'if-curie-v2',\n",
       " 'if-davinci:3.0.0',\n",
       " 'davinci-if:3.0.0',\n",
       " 'davinci-instruct-beta:2.0.0',\n",
       " 'text-ada:001',\n",
       " 'text-davinci:001',\n",
       " 'text-curie:001',\n",
       " 'text-babbage:001']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"Open the pod bay doors please.\"\n",
    "available_models = openai_connector.list_models()\n",
    "available_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "trIAge = TrIAge(\n",
    "    model_provider=\"openai\",\n",
    "    api_key=get_secret(\"openai_token\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am trIAge, a chatbot designed to assist users and maintainers of open source projects. My main goal is to help with issue triage, which involves analyzing and categorizing reported issues to ensure that they are addressed in a timely and effective manner. I can also provide information on open source best practices, community management, and other topics related to maintaining a healthy and thriving open source project.\n"
     ]
    }
   ],
   "source": [
    "trIAge.tell(\"Who are you?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As a chatbot, I can perform a variety of functions related to issue triage and open source project management. Here are some examples:\n",
      "\n",
      "- Analyze and categorize reported issues based on their severity, impact, and other factors\n",
      "- Provide guidance on how to structure issue templates and bug reports for maximum effectiveness\n",
      "- Offer suggestions for how to prioritize and assign issues to team members or volunteers\n",
      "- Provide information on open source best practices, including how to build a welcoming and inclusive community around an open source project\n",
      "- Help maintainers create effective documentation and guides to help users get started with using and contributing to their project\n",
      "- Suggest tools and resources that can help with project management, such as bug-tracking software or CI/CD solutions\n",
      "\n",
      "If there's something specific you need help with, just let me know!\n"
     ]
    }
   ],
   "source": [
    "trIAge.tell(\"What can you do?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidRequestError",
     "evalue": "The model: `gpt-4-32k-0314` does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidRequestError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m trIAge \u001b[39m=\u001b[39m TrIAge(\n\u001b[1;32m      2\u001b[0m     model_provider\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mopenai\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m      3\u001b[0m     model_name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mgpt-4-32k-0314\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m      4\u001b[0m     api_key\u001b[39m=\u001b[39;49mget_secret(\u001b[39m\"\u001b[39;49m\u001b[39mopenai_token\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m      5\u001b[0m )\n",
      "Cell \u001b[0;32mIn[18], line 18\u001b[0m, in \u001b[0;36mTrIAge.__init__\u001b[0;34m(self, model_provider, api_key, model_name)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mset_api_key(api_key)\n\u001b[1;32m     17\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchat_history \u001b[39m=\u001b[39m []\n\u001b[0;32m---> 18\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconfigure()\n",
      "Cell \u001b[0;32mIn[18], line 35\u001b[0m, in \u001b[0;36mTrIAge.configure\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchat_history \u001b[39m=\u001b[39m []\n\u001b[1;32m     29\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchat_history\u001b[39m.\u001b[39mappend(\n\u001b[1;32m     30\u001b[0m     {\n\u001b[1;32m     31\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mrole\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39msystem\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     32\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mcontent\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mYou are \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mname\u001b[39m}\u001b[39;00m\u001b[39m, a helpful bot that assists users and maintainers of open source projects.\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     33\u001b[0m     }\n\u001b[1;32m     34\u001b[0m )\n\u001b[0;32m---> 35\u001b[0m openai\u001b[39m.\u001b[39;49mChatCompletion\u001b[39m.\u001b[39;49mcreate(\n\u001b[1;32m     36\u001b[0m     model\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel_name,\n\u001b[1;32m     37\u001b[0m     messages\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mchat_history\n\u001b[1;32m     38\u001b[0m )\n",
      "File \u001b[0;32m~/miniforge3/envs/trIAge/lib/python3.10/site-packages/openai/api_resources/chat_completion.py:25\u001b[0m, in \u001b[0;36mChatCompletion.create\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m     24\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 25\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mcreate(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     26\u001b[0m     \u001b[39mexcept\u001b[39;00m TryAgain \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     27\u001b[0m         \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m time\u001b[39m.\u001b[39mtime() \u001b[39m>\u001b[39m start \u001b[39m+\u001b[39m timeout:\n",
      "File \u001b[0;32m~/miniforge3/envs/trIAge/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py:153\u001b[0m, in \u001b[0;36mEngineAPIResource.create\u001b[0;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[1;32m    128\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcreate\u001b[39m(\n\u001b[1;32m    129\u001b[0m     \u001b[39mcls\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams,\n\u001b[1;32m    137\u001b[0m ):\n\u001b[1;32m    138\u001b[0m     (\n\u001b[1;32m    139\u001b[0m         deployment_id,\n\u001b[1;32m    140\u001b[0m         engine,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    150\u001b[0m         api_key, api_base, api_type, api_version, organization, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams\n\u001b[1;32m    151\u001b[0m     )\n\u001b[0;32m--> 153\u001b[0m     response, _, api_key \u001b[39m=\u001b[39m requestor\u001b[39m.\u001b[39;49mrequest(\n\u001b[1;32m    154\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mpost\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    155\u001b[0m         url,\n\u001b[1;32m    156\u001b[0m         params\u001b[39m=\u001b[39;49mparams,\n\u001b[1;32m    157\u001b[0m         headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m    158\u001b[0m         stream\u001b[39m=\u001b[39;49mstream,\n\u001b[1;32m    159\u001b[0m         request_id\u001b[39m=\u001b[39;49mrequest_id,\n\u001b[1;32m    160\u001b[0m         request_timeout\u001b[39m=\u001b[39;49mrequest_timeout,\n\u001b[1;32m    161\u001b[0m     )\n\u001b[1;32m    163\u001b[0m     \u001b[39mif\u001b[39;00m stream:\n\u001b[1;32m    164\u001b[0m         \u001b[39m# must be an iterator\u001b[39;00m\n\u001b[1;32m    165\u001b[0m         \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(response, OpenAIResponse)\n",
      "File \u001b[0;32m~/miniforge3/envs/trIAge/lib/python3.10/site-packages/openai/api_requestor.py:226\u001b[0m, in \u001b[0;36mAPIRequestor.request\u001b[0;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrequest\u001b[39m(\n\u001b[1;32m    206\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    207\u001b[0m     method,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    214\u001b[0m     request_timeout: Optional[Union[\u001b[39mfloat\u001b[39m, Tuple[\u001b[39mfloat\u001b[39m, \u001b[39mfloat\u001b[39m]]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    215\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[Union[OpenAIResponse, Iterator[OpenAIResponse]], \u001b[39mbool\u001b[39m, \u001b[39mstr\u001b[39m]:\n\u001b[1;32m    216\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrequest_raw(\n\u001b[1;32m    217\u001b[0m         method\u001b[39m.\u001b[39mlower(),\n\u001b[1;32m    218\u001b[0m         url,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    224\u001b[0m         request_timeout\u001b[39m=\u001b[39mrequest_timeout,\n\u001b[1;32m    225\u001b[0m     )\n\u001b[0;32m--> 226\u001b[0m     resp, got_stream \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_interpret_response(result, stream)\n\u001b[1;32m    227\u001b[0m     \u001b[39mreturn\u001b[39;00m resp, got_stream, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapi_key\n",
      "File \u001b[0;32m~/miniforge3/envs/trIAge/lib/python3.10/site-packages/openai/api_requestor.py:619\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response\u001b[0;34m(self, result, stream)\u001b[0m\n\u001b[1;32m    611\u001b[0m     \u001b[39mreturn\u001b[39;00m (\n\u001b[1;32m    612\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_interpret_response_line(\n\u001b[1;32m    613\u001b[0m             line, result\u001b[39m.\u001b[39mstatus_code, result\u001b[39m.\u001b[39mheaders, stream\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    614\u001b[0m         )\n\u001b[1;32m    615\u001b[0m         \u001b[39mfor\u001b[39;00m line \u001b[39min\u001b[39;00m parse_stream(result\u001b[39m.\u001b[39miter_lines())\n\u001b[1;32m    616\u001b[0m     ), \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    617\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    618\u001b[0m     \u001b[39mreturn\u001b[39;00m (\n\u001b[0;32m--> 619\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_interpret_response_line(\n\u001b[1;32m    620\u001b[0m             result\u001b[39m.\u001b[39;49mcontent\u001b[39m.\u001b[39;49mdecode(\u001b[39m\"\u001b[39;49m\u001b[39mutf-8\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m    621\u001b[0m             result\u001b[39m.\u001b[39;49mstatus_code,\n\u001b[1;32m    622\u001b[0m             result\u001b[39m.\u001b[39;49mheaders,\n\u001b[1;32m    623\u001b[0m             stream\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    624\u001b[0m         ),\n\u001b[1;32m    625\u001b[0m         \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    626\u001b[0m     )\n",
      "File \u001b[0;32m~/miniforge3/envs/trIAge/lib/python3.10/site-packages/openai/api_requestor.py:682\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response_line\u001b[0;34m(self, rbody, rcode, rheaders, stream)\u001b[0m\n\u001b[1;32m    680\u001b[0m stream_error \u001b[39m=\u001b[39m stream \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39merror\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m resp\u001b[39m.\u001b[39mdata\n\u001b[1;32m    681\u001b[0m \u001b[39mif\u001b[39;00m stream_error \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39m200\u001b[39m \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m rcode \u001b[39m<\u001b[39m \u001b[39m300\u001b[39m:\n\u001b[0;32m--> 682\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandle_error_response(\n\u001b[1;32m    683\u001b[0m         rbody, rcode, resp\u001b[39m.\u001b[39mdata, rheaders, stream_error\u001b[39m=\u001b[39mstream_error\n\u001b[1;32m    684\u001b[0m     )\n\u001b[1;32m    685\u001b[0m \u001b[39mreturn\u001b[39;00m resp\n",
      "\u001b[0;31mInvalidRequestError\u001b[0m: The model: `gpt-4-32k-0314` does not exist"
     ]
    }
   ],
   "source": [
    "trIAge = TrIAge(\n",
    "    model_provider=\"openai\",\n",
    "    model_name=\"gpt-4-32k-0314\",\n",
    "    api_key=get_secret(\"openai_token\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trIAge",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
