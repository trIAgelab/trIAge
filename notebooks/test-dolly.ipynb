{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dolly Test\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preamble\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cls/miniforge3/envs/trIAge/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import torch\n",
    "from transformers import pipeline\n",
    "from langchain.llms.base import LLM\n",
    "from llama_index import SimpleDirectoryReader, LangchainEmbedding, GPTVectorStoreIndex, PromptHelper, LLMPredictor, ServiceContext\n",
    "from llama_index.langchain_helpers.text_splitter import TokenTextSplitter\n",
    "from llama_index.node_parser.simple import SimpleNodeParser\n",
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workflow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: sentence-transformers/all-mpnet-base-v2\n",
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device: cpu\n",
      "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:01<00:00,  1.12s/it]\n",
      "INFO:llama_index.token_counter.token_counter:> [build_index_from_nodes] Total LLM token usage: 0 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [build_index_from_nodes] Total embedding token usage: 1346 tokens\n"
     ]
    }
   ],
   "source": [
    "INPUT_FOLDER = \"../data/\"\n",
    "\n",
    "index_files = list(Path(INPUT_FOLDER).glob(\"*\"))\n",
    "\n",
    "max_input_size = 2048\n",
    "num_output = 256\n",
    "max_chunk_overlap = 20\n",
    "prompt_helper = PromptHelper(max_input_size, num_output, max_chunk_overlap)\n",
    "\n",
    "\n",
    "pipe = pipeline(\"text-generation\", model=\"databricks/dolly-v2-3b\", trust_remote_code=True, torch_dtype=torch.bfloat16, device_map=\"auto\")\n",
    "embed_model = LangchainEmbedding(HuggingFaceEmbeddings())\n",
    "\n",
    "class CustomLLM(LLM):\n",
    "    model_name = \"databricks/dolly-v2-3b\"\n",
    "\n",
    "    def _call(self, prompt, stop = None):\n",
    "        response = pipe(prompt, max_new_tokens=num_output)[0][\"generated_text\"]\n",
    "        return response\n",
    "\n",
    "    @property\n",
    "    def _identifying_params(self):\n",
    "        return {\"name_of_model\": self.model_name}\n",
    "\n",
    "    @property\n",
    "    def _llm_type(self):\n",
    "        return \"custom\"\n",
    "\n",
    "# define our LLM\n",
    "llm_predictor = LLMPredictor(llm=CustomLLM())\n",
    "\n",
    "node_parser = SimpleNodeParser(text_splitter=TokenTextSplitter(chunk_size=512, chunk_overlap=max_chunk_overlap))\n",
    "prompt_helper = PromptHelper(max_input_size, num_output, max_chunk_overlap)\n",
    "service_context = ServiceContext.from_defaults(llm_predictor=llm_predictor, embed_model=embed_model, prompt_helper=prompt_helper, node_parser=node_parser, chunk_size_limit=512)\n",
    "# Load your data\n",
    "documents = SimpleDirectoryReader(input_files=index_files).load_data()\n",
    "\n",
    "index = GPTVectorStoreIndex.from_documents(documents, service_context=service_context)\n",
    "\n",
    "query_engine = index.as_query_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  4.71it/s]\n",
      "INFO:llama_index.token_counter.token_counter:> [retrieve] Total LLM token usage: 0 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [retrieve] Total embedding token usage: 8 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [get_response] Total LLM token usage: 295 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [get_response] Total embedding token usage: 0 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Response(response='Documentation is a project to automatically generate documentation from the code of a project.', source_nodes=[NodeWithScore(node=Node(text='\\n\\nTarget Functionality\\n\\n- Support: answer questions based on documentation, generate explanatory code examples\\n- Issue Quality Control: analyze issues for quality of description and reproducibility, provide feedback for improvement\\n- Issue Triage: Automatically categorize issues by content (feature request, bug, support) and tag them, detect duplicates, prioritize issues (e.g. security issues), link related issues\\n- Debugging: Analyze error causes, generate solution suggestions\\n- Testing: automatic generation of test cases based on issues\\n- Pull Request Review: Analysis of pull requests regarding code quality, code style, documentation, test coverage, etc.\\n- Documentation: Generation of documentation proposals from the project context\\n- Changelogs: Generation of changelogs from the project history\\n\\n\\n', doc_id='849dab44-1d60-4e30-b41e-208524d9607e', embedding=None, doc_hash='66b64d2cd6ae8494f664315019fd682fe1a7be55dc29d7d658cffbc68ed92cc4', extra_info=None, node_info={'start': 0, 'end': 811}, relationships={<DocumentRelationship.SOURCE: '1'>: 'd0dc1af0-78d8-4e3d-b360-8ac4c61de4c2'}), score=0.2863370372980722), NodeWithScore(node=Node(text='\\n\\nSetup\\n\\n1. Clone the repository\\n2. Install the dependencies from `requirements.txt`\\n3. Obtain a GitHub API token and store it in the first line of a file called `secrets/github_token.txt` \\n4. Obtain an OpenAI API token and store it in the first line of a file called `secrets/openai_token.txt`\\n  \\n', doc_id='23081bd2-016c-450d-b6e8-61067efd9754', embedding=None, doc_hash='b733b54bf9383c27f6d42495caf4e15af4342adc1f3b0bcf27f99585d32b764f', extra_info=None, node_info={'start': 0, 'end': 298}, relationships={<DocumentRelationship.SOURCE: '1'>: '5c757a7f-a0f6-4890-97e6-f8ab047ce1e0'}), score=0.1937419655344816)], extra_info={'849dab44-1d60-4e30-b41e-208524d9607e': None, '23081bd2-016c-450d-b6e8-61067efd9754': None})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_engine.query(\"What is the title of the project?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  9.77it/s]\n",
      "INFO:llama_index.token_counter.token_counter:> [retrieve] Total LLM token usage: 0 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [retrieve] Total embedding token usage: 6 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [get_response] Total LLM token usage: 553 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [get_response] Total embedding token usage: 0 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Response(response='The new version of the data generator would be compatible with the gpt-3.5-turbo model.', source_nodes=[NodeWithScore(node=Node(text='\\n\\nTarget Functionality\\n\\n- Support: answer questions based on documentation, generate explanatory code examples\\n- Issue Quality Control: analyze issues for quality of description and reproducibility, provide feedback for improvement\\n- Issue Triage: Automatically categorize issues by content (feature request, bug, support) and tag them, detect duplicates, prioritize issues (e.g. security issues), link related issues\\n- Debugging: Analyze error causes, generate solution suggestions\\n- Testing: automatic generation of test cases based on issues\\n- Pull Request Review: Analysis of pull requests regarding code quality, code style, documentation, test coverage, etc.\\n- Documentation: Generation of documentation proposals from the project context\\n- Changelogs: Generation of changelogs from the project history\\n\\n\\n', doc_id='849dab44-1d60-4e30-b41e-208524d9607e', embedding=None, doc_hash='66b64d2cd6ae8494f664315019fd682fe1a7be55dc29d7d658cffbc68ed92cc4', extra_info=None, node_info={'start': 0, 'end': 811}, relationships={<DocumentRelationship.SOURCE: '1'>: 'd0dc1af0-78d8-4e3d-b360-8ac4c61de4c2'}), score=0.542857601214301), NodeWithScore(node=Node(text='the gpt-3.5-turbo model would be used with the data generator, and what sorts of functionality you would like the new version of the data generator to have. Additionally, if there are any relevant research or use cases that might inform the desired functionality of the new feature, please share those as well. The more information and context you can provide, the better we can evaluate your request and create a solution that meets your needs.\\n\\n\\n\\n\\n> **Suggest how to resolve the issue**\\n\\n> Based on the information provided, it seems that the user is requesting a new version of the data generator that is compatible with the gpt-3.5-turbo model. Here is a possible way to resolve the issue:\\n>1. Research the capabilities of the gpt-3.5-turbo model and determine what modifications would need to be made to the existing data generator to support this model.\\n>2. Based on the research, develop a set of requirements for the new version of the data generator that would support the gpt-3.5-turbo model.\\n>3. Implement the necessary modifications to the data generator code to support the new model, ensuring that the new code meets the requirements developed in step two.\\n>4. Test the new version of the data generator with the gpt-3.5-turbo model, and make any necessary adjustments to ensure that the generated data meets the desired quality > standards.\\n> 5. Release the new version of the data generator as an update to the existing code base, including thorough documentation about the changes made and the requirements for use.\\n> \\n\\n\\n', doc_id='d0f47675-8369-4936-bd78-3b061a34b4bb', embedding=None, doc_hash='ca8e6f2a043bf6d93fd9b986ec6d9de8aa5b16632f31923720d6bd4c8d0cf24a', extra_info=None, node_info={'start': 1816, 'end': 3354}, relationships={<DocumentRelationship.SOURCE: '1'>: '83383309-eeea-4669-97b8-4747f3244180', <DocumentRelationship.PREVIOUS: '2'>: '87cba8a5-22c5-4d8d-bfb5-dda6b9e89da0'}), score=0.19737885017160425)], extra_info={'849dab44-1d60-4e30-b41e-208524d9607e': None, 'd0f47675-8369-4936-bd78-3b061a34b4bb': None})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_engine.query(\"What is the target functionality?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trIAge",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
